{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKPOINT 0: Create GCP instance w/ GPU and nice CPUs, fork test-gcp-transfer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.unpack_archive(\"RealData.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import Model\n",
    "from Losses import Losses\n",
    "from ModelEvaluation import ModelEvaluation\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from DataGenerator import DataGenerator\n",
    "from TrainingManager import TrainingManager\n",
    "from VideoGenerator import VideoGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = VideoGenerator(topNm = 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-de56df1af0d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanImages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerateVideos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/SageMaker/test-gcp-transfer/ConditionalGAN/VideoGenerator.py\u001b[0m in \u001b[0;36mgenerateVideos\u001b[0;34m(self, numVideos, variance)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mfinalImages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchImageFromDepth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcircleTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiameter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcircleTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiameter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mfinalImages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_eyelashes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinalImages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mfinalCleanImages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetchImageFromDepth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/test-gcp-transfer/ConditionalGAN/VideoGenerator.py\u001b[0m in \u001b[0;36madd_eyelashes\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_eyelashes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mim_pil\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n"
     ]
    }
   ],
   "source": [
    "images, cleanImages, maps = generator.generateVideos(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auxSlice(content, channels):\n",
    "    numVid = content.shape[0]\n",
    "    numFrame = content.shape[1]\n",
    "    width = content.shape[2]\n",
    "    height = content.shape[3]\n",
    "    \n",
    "    \n",
    "    slices = np.zeros((numVid, numFrame*width, height, channels))\n",
    "        \n",
    "    for i in range(numVid):\n",
    "        stack = []\n",
    "        for k in range(numFrame):\n",
    "            stack.append(content[i][k].reshape(width, height, channels))\n",
    "        slices[i] = np.vstack(stack)\n",
    "        \n",
    "        \n",
    "    return slices\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import heapq\n",
    "import csv\n",
    "from matplotlib import cm\n",
    "\n",
    "def showImage(image):\n",
    "    data = np.reshape(image, (image.shape[0],image.shape[1],3))\n",
    "    plt.imshow(imRegulate(data), interpolation='nearest')\n",
    "    return plt.show()\n",
    "    \n",
    "    \n",
    "def showMap(depthMap):\n",
    "\n",
    "    d1 = depthMap.shape[0]\n",
    "    d2 = depthMap.shape[1]\n",
    "    \n",
    "    pData = np.reshape(depthMap, (d1,d2))\n",
    "\n",
    "    fig = plt.figure(figsize = (5,5))\n",
    "    ax1 = fig.add_subplot(projection='3d')\n",
    "\n",
    "    x = np.arange(0, d1)\n",
    "    y = np.arange(0, d2)\n",
    "    \n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    surf = ax1.plot_surface(X, Y, pData, cmap = cm.coolwarm)\n",
    "\n",
    "    ax1.set_zlabel(\"Thickness\" + \" (\" + \"nm\" + \")\")\n",
    "    ax1.set_title('Patient Tear Film Thickness Profile')\n",
    "\n",
    "\n",
    "\n",
    "    return plt.show()\n",
    "    \n",
    "        \n",
    "def imRegulate(data):\n",
    "    m = np.max(data)\n",
    "    mi = np.min(data)\n",
    "    norm = ((data - mi) / (m - mi))*255\n",
    "    return norm.astype(np.uint8)\n",
    "\n",
    "\n",
    "def tRegulate(data):\n",
    "    data = tf.truediv(\n",
    "       tf.subtract(\n",
    "          data, \n",
    "          tf.reduce_min(data)\n",
    "       ), \n",
    "       tf.subtract(\n",
    "          tf.reduce_max(data), \n",
    "          tf.reduce_min(data)\n",
    "       )\n",
    "    )\n",
    "    return tf.math.multiply(data,5000)\n",
    "\n",
    "@tf.custom_gradient\n",
    "def tCycle(depth):\n",
    "    depth = tRegulate(depth)\n",
    "    depth = tf.convert_to_tensor(depth)\n",
    "    depth = tf.cast(tf.reshape(depth, (48*48)), tf.int32).numpy()\n",
    "    print(depth)\n",
    "\n",
    "    def grad(upstream):\n",
    "            values = dColorMap[[depth]]\n",
    "            ret = tf.convert_to_tensor(values.reshape(1,48,48,3).astype(np.float32))\n",
    "            return ret * upstream\n",
    "    \n",
    "    values = colorMap[[depth]]\n",
    "    return tf.convert_to_tensor(values.reshape(1,48,48,3).astype(np.float32)), grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"/opt/conda/lib/python3.7/site-packages/models/images/\"\n",
    "counter = 1\n",
    "for f in os.listdir(path):\n",
    "    suffix = f.split('.')[-1]\n",
    "    if suffix == 'jpg' or suffix == 'png':\n",
    "        new = '{}.{}'.format(str(counter), suffix)\n",
    "        os.rename(path + f, path + new)\n",
    "        counter = int(counter) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = Losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = Database() #CHECKPOINT 1: Compile to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisyRGB = {\n",
    "    \"gen_down\" : [512,1024,2048],\n",
    "    \"gen_up\": [512,1024,2048],\n",
    "    \"gen_dropout\" : [True, False, False],\n",
    "    \"disc_down\": [32, 64, 128, 256],\n",
    "    \"gen_lr\": 0.002,\n",
    "    \"gen_b1\": 0.5,\n",
    "    \"disc_lr\" : 0.002,\n",
    "    \"disc_b1\": 0.5,\n",
    "    \"epochs\": 50,\n",
    "    \"time\": database.fetchDate(),\n",
    "    \"name\" : \"longtest10/\",\n",
    "    \"hsi\" : False,\n",
    "    \"numPerlin\": 5000,\n",
    "    \"numGaussian\": 20000,\n",
    "    \"topNm\": 2000,\n",
    "    \"notes\": \"Noisy, 0.1\",\n",
    "    \"optimalCheckpoint\": 10,\n",
    "    \"std\": 0.1,\n",
    "    \"d1\": 1728,\n",
    "    \"d2\": 144\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidRGB = {\n",
    "    \"gen_down\" : [128,256,512],\n",
    "    \"gen_up\": [128,256,512],\n",
    "    \"gen_dropout\" : [True, True, False],\n",
    "    \"disc_down\": [32],\n",
    "    \"gen_lr\": 0.0002,\n",
    "    \"gen_b1\": 0.5,\n",
    "    \"disc_lr\" : 0.0002,\n",
    "    \"disc_b1\": 0.5,\n",
    "    \"epochs\": 6,\n",
    "    \"time\": database.fetchDate(),\n",
    "    \"name\" : \"longtest1\",\n",
    "    \"topNm\": 5000,\n",
    "    \"notes\": \"Noisy, 0.1\",\n",
    "    \"optimalCheckpoint\": 10,\n",
    "    \"numVid\": 10, \n",
    "    \"d1\": 1728, \n",
    "    \"d2\": 144\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisyRGBManager = TrainingManager(noisyRGB, auxSlice(images, 3), auxSlice(maps, 1), auxSlice(images, 3) ,auxSlice(maps, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisyRGBManager.previewGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisyRGBManager.previewDiscriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisyRGBManager.fit() #CHECKPOINT 2: Finish a training session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKPOINT 3: Create some testing infrastructure. --> Run some testing videos, get output, and display output. (One at a time for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "pix2pix.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m94"
  },
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
